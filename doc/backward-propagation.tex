\documentclass[12pt]{article}

\usepackage{fullpage,amsmath,amsthm,amssymb}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\cost}{cost}

\newcommand{\R}{\mathbb{R}}

\newcommand{\bsigma}{\pmb{\sigma}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\e}{\mathbf{e}}
\renewcommand{\L}{\mathbf{L}}

\title{Gradient Descent for Neural Nets}
\author{D\'avid P\'al}

\begin{document}

\maketitle

\section{Logistic Sigmoid}

Logistic sigmoid function is $\sigma:\R \to (0,1)$,
$$
\sigma(z) = \frac{1}{1 + e^{-z}} \qquad z \in \R \; .
$$
Its derivative is
$$
\sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \sigma(z) (1 - \sigma(z)) \; .
$$
Vector logistic sigmoid is $\bsigma:\R^d \to (0,1)^d$ is
$$
\bsigma(\z) = \bsigma(z_1, z_2, \dots, z_d) = (\sigma(z_1), \sigma(z_2), \dots, \sigma(z_d)) \; .
$$
Its derivative (Jacobian) is
\begin{align*}
\frac{d \bsigma(\z)}{d\z}
& = \diag(\sigma'(z_1), \sigma'(z_2), \dots, \sigma'(z_d)) \\
& = \diag(\sigma(z_1) (1 - \sigma(z_1)), \sigma(z_2) (1 - \sigma(z_2)), \dots, \sigma(z_d) (1 - \sigma(z_d))) \; .
\end{align*}

\section{Logarithmic Loss}

Logarithmic loss (also called cross entropy) is $L:[0,1] \times (0,1) \to \R$,
$$
L(y,p) = - y \ln(p) - (1-y) \ln(1-p) \; .
$$
The first argument is the correct label, the second argument
is the prediction. Its partial derivative with respect to the prediction is
$$
\frac{\partial L(y,p)}{\partial p} = - \frac{y}{p} + \frac{1-y}{1-p} = \frac{-y(1-p) + p(1-y)}{p(1-p)} = \frac{p-y}{p(1-p)} \; .
$$
Vector log loss is $\L:[0,1]^d \times (0,1)^d \to \R$
$$
\L(\y,\p) = \L(y_1, y_2, \dots, y_d, p_1, p_2, \dots, p_d) = \sum_{i=1}^d L(y_i, p_i) \; .
$$
Its partial derivate (gradient) with respect to the prediction is
$$
\frac{\partial \L(\y, \p)}{\partial p}
= \left(\frac{\partial L(y_1,p_1)}{\partial p_1}, \frac{\partial L(y_2,p_2)}{\partial p_2}, \dots, \frac{\partial L(y_d,p_d)}{\partial p_d} \right)
$$

\section{Logistic Loss}

To simplify things, we define composition of $\L$ and $\sigma$.
Formally, $g:\R^d \times \R^d \to \R$,
$$
g(\z) = \L(\y, \bsigma(\z)) = \sum_{i=1}^d L(y_i, \sigma(z_i)) \; .
$$
The function $g$ is the (vector) logistic loss.
Its partial derivative with respect to $z_i$ is
\begin{align*}
\frac{\partial g(\y, \z)}{\partial z_i}
& = \frac{\partial L(y_i, \sigma(z_i))}{\partial z_i} \\
& = \frac{\partial L(y_i, p)}{\partial z_i} \bigg|_{p=\sigma(z_i)} \cdot \frac{\sigma(z_i)}{\partial z_i} \\
& = \frac{\sigma(z_i) - y_i}{\sigma(z_i)(1-\sigma(z_i))} \cdot  \sigma(z_i) (1 - \sigma(z_i)) \\
& =  \sigma(z_i) - y_i \; .
\end{align*}
Thus gradient with respect to $\z$ is
$$
\frac{\partial g(\y, \z)}{\partial \z} = \bsigma(\z) - \y  \; .
$$

\section{Derivative of Matrix-Vector Multiplication}

Suppose $f(A,x) = A \cdot x$ where $A$ is a $m \times n$ matrix and $x \in \R^n$.
The partial derivatives are
$$
\frac{\partial f(A,x)}{\partial A_{i,j}} = x_j \e_i \qquad \text{(for $i=1,2,\dots,m$ and $j=1,2,\dots,n$)} \,
$$
where $\e_i \in \R^m$ is the $i$-th vector of the standard bases of $\R^m$.

\section{Neural Network}

We consider two-layer neural network with input $x \in \R^n$, hidden layer of size $m$, and $k$ outputs.
The network is parametrized by $m \times n$ matrix $A$, bias vector $b \in \R^m$, $k \times m$ matrix $C$ and bias vector $d \in \R^k$.
$$
f(A,b,C,d,x) = \bsigma(C \cdot \bsigma(A \cdot x + b) + d) \; .
$$
The goal of the network is minimize log loss with respect to correct label.
$$
\cost_{x,\y}(A,b,C,d) = \L(\y, f(A,b,C,d,x)) = \L(\y, \bsigma(C \cdot \bsigma(A \cdot x + b) + d)) = g(\y, C \cdot \bsigma(A \cdot x + b) + d) \; .
$$
The algorithm for minimizing the cost is gradient descent. Hence, we need to
compute the partial derivatives of $\cost$ with respect to the parameters:

\begin{align*}
\frac{\partial \cost_{x,\y}(A,b,C,d)}{\partial A_{i,j}}
& = \frac{\partial g(\y, C \cdot \bsigma(A \cdot x + b) + d)}{\partial A_{i,j}} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ \frac{\partial [C \cdot \bsigma(A \cdot x + b) + d]}{\partial A_{i,j}} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ C \cdot \frac{\partial \bsigma(A \cdot x + b)}{\partial A_{i,j}} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ C \cdot \frac{\partial \bsigma(\z)}{\partial \z} \bigg|_{\z=A \cdot x + b} \frac{\partial [A \cdot x + b]}{\partial A_{i,j}} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ C \cdot \frac{\partial \bsigma(\z)}{\partial \z} \bigg|_{\z=A \cdot x + b} \cdot x_j \e_i \\
& = \left(\bsigma(C \cdot \bsigma(A \cdot x + b) + d)) - \y \right) \cdot C \cdot \frac{\partial \bsigma(\z)}{\partial \z} \bigg|_{\z=A \cdot x + b} \cdot x_j \e_i \; .
\end{align*}

\begin{align*}
\frac{\partial \cost_{x,\y}(A,b,C,d)}{\partial b}
& = \frac{\partial g(C \cdot \bsigma(A \cdot x + b) + d)}{\partial b} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ \frac{\partial [C \cdot \bsigma(A \cdot x + b) + d]}{\partial b} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ C \cdot \frac{\partial \bsigma(A \cdot x + b)}{\partial b} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ C \cdot \frac{\partial \bsigma(\z)}{\partial \z} \bigg|_{\z=A \cdot x + b} \frac{\partial [A \cdot x + b]}{\partial b} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ C \cdot \frac{\partial \bsigma(\z)}{\partial \z} \bigg|_{\z=A \cdot x + b} \cdot I_{m \times m} \\
& = \left(\bsigma(C \cdot \bsigma(A \cdot x + b) + d))  - \y \right) \cdot \ C \cdot \frac{\partial \bsigma(\z)}{\partial \z} \bigg|_{\z=A \cdot x + b} \; .
\end{align*}

\begin{align*}
\frac{\partial \cost_{x,\y}(A,b,C,d)}{\partial C_{i,j}}
& = \frac{\partial g(\y, C \cdot \bsigma(A \cdot x + b) + d)}{\partial C_{i,j}} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ \frac{\partial [C \cdot \bsigma(A \cdot x + b) + d]}{\partial C_{i,j}} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ [\bsigma(A \cdot x + b)]_j \cdot \e_i \\
& = \left(\bsigma(C \cdot \bsigma(A \cdot x + b) + d))  - \y \right) \cdot [\bsigma(A \cdot x + b)]_j \cdot \e_i \\
& = \left[\bsigma(C \cdot \bsigma(A \cdot x + b) + d))  - \y \right]_i \cdot [\bsigma(A \cdot x + b)]_j \; .
\end{align*}

\begin{align*}
\frac{\partial \cost_{x,\y}(A,b,C,d)}{\partial d}
& = \frac{\partial g(C \cdot \bsigma(A \cdot x + b) + d)}{\partial d} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ \frac{\partial [C \cdot \bsigma(A \cdot x + b) + d]}{\partial d} \\
& = \frac{\partial g(\y, \z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)} \ I_{k \times k} \\
& = \bsigma(C \cdot \bsigma(A \cdot x + b) + d)) - \y \; .
\end{align*}

The backward propagation algorithm is simply the (online stochastic) gradient descent algorithm.
In round $t$ it takes sample $(x^{(t)}, \y^{(t)})$ and updates the parameters:
\begin{align*}
A^{(t+1)} & = A^{(t)} - \frac{\partial \cost_{x^{(t)},\y^{(t)}}(A,b,C,d)}{\partial A} \; , \\
b^{(t+1)} & = b^{(t)} - \frac{\partial \cost_{x^{(t)},\y^{(t)}}(A,b,C,d)}{\partial b} \; , \\
C^{(t+1)} & = C^{(t)} - \frac{\partial \cost_{x^{(t)},\y^{(t)}}(A,b,C,d)}{\partial C} \; , \\
d^{(t+1)} & = d^{(t)} - \frac{\partial \cost_{x^{(t)},\y^{(t)}}(A,b,C,d)}{\partial d} \; .
\end{align*}

\end{document}
