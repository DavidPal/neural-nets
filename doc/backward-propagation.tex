\documentclass[12pt]{article}

\usepackage{fullpage,amsmath,amsthm,amssymb}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\cost}{cost}

\newcommand{\R}{\mathbb{R}}

\newcommand{\bsigma}{\pmb{\sigma}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\e}{\mathbf{e}}
\renewcommand{\L}{\mathbf{L}}

\title{Gradient Descent for Neural Nets}
\author{D\'avid P\'al}

\begin{document}

\maketitle

We start by defining several functions.
Logistic sigmoid function is $\sigma:\R \to (0,1)$,
$$
\sigma(z) = \frac{1}{1 + e^{-z}} \qquad z \in \R \; .
$$
Its derivative is
$$
\sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \sigma(z) (1 - \sigma(z)) \; .
$$

Log loss (also called cross entropy) is $L:[0,1] \times (0,1) \to \R$,
$$
L(y,p) = - y \ln(p) - (1-y) \ln(1-p) \; .
$$
The first argument is the correct label, the second argument
is the prediction. Its partial derivative with respect to the prediction is,
$$
\frac{\partial L(y,p)}{\partial p} = - \frac{y}{p} + \frac{1-y}{1-p} \; .
$$

We generalize both the sigmoid and the log loss to vector arguments.
Vector logistic sigmoid is $\bsigma:\R^d \to (0,1)^d$ is
$$
\bsigma(\z) = \bsigma(z_1, z_2, \dots, z_d) = (\sigma(z_1), \sigma(z_2), \dots, \sigma(z_d)) \; .
$$
Its derivative (Jacobian) is
\begin{align*}
\frac{d \bsigma(\z)}{d\z}
& = \diag(\sigma'(z_1), \sigma'(z_2), \dots, \sigma'(z_d)) \\
& = \diag(\sigma(z_1) (1 - \sigma(z_2)), \sigma(z_2) (1 - \sigma(z_2)), \dots, \sigma(z_d) (1 - \sigma(z_d))) \; .
\end{align*}
Vector log loss is $\L:[0,1]^d \times (0,1)^d \to \R$
$$
\L(\y,\p) = \L(y_1, y_2, \dots, y_d, p_1, p_2, \dots, p_d) = \sum_{i=1}^d L(y_i, p_i) \; .
$$
Its partial derivate (gradient) with respect to the prediction is
$$
\frac{d \L(\y, \p)}{dp}
= \left(\frac{\partial L(y_1,p_1)}{\partial p_1}, \frac{\partial L(y_2,p_2)}{\partial p_2}, \dots, \frac{\partial L(y_d,p_d)}{\partial p_d} \right)
$$

\section{Derivative of Matrix-Vector Multiplication}

Suppose $f(A,x) = A \cdot x$ where $A$ is a $m \times n$ matrix and $x \in \R^n$.
The partial derivatives are
$$
\frac{\partial f(A,x)}{\partial A_{i,j}} = x_j \e_i \qquad \text{(for $i=1,2,\dots,m$ and $j=1,2,\dots,n$)} \,
$$
where $\e_i \in \R^m$ is the $i$-th vector of the standard bases of $\R^m$.

\section{Neural Network}

We consider two-layer neural network with input $x \in \R^n$, hidden layer of size $m$, and $k$ outputs.
The network is parametrized by $m \times n$ matrix $A$, bias vector $b \in \R^m$, $k \times m$ matrix $C$ and bias vector $d \in \R^k$.
$$
f(A,b,C,d,x) = \bsigma(C \cdot \bsigma(A \cdot x + b) + d) \; .
$$
The goal of the network is minimize log loss with respect to correct label.
$$
\cost(A,b,C,d) = \L(\y, f(A,b,C,d,x)) = \L(\y, \bsigma(C \cdot \bsigma(A \cdot x + b) + d)) \; .
$$

The algorithm for minimizing the cost is gradient descent. Hence, we
compute the partial derivatives of $\cost$.
\begin{align*}
& \frac{\partial \cost(A,b,C,d)}{\partial C_{i,j}} \\
& = \frac{\partial \L(\y, f(A,b,C,d,x))}{\partial C_{i,j}}  \\
& = \frac{\partial \L(\y, \bsigma(C \cdot \bsigma(A \cdot x + b) + d))}{\partial C_{i,j}} \\
& = \frac{\partial \L(\y, \p)}{\partial \p}\bigg|_{\p=\bsigma(C \cdot \bsigma(A \cdot x + b) + d))} \ \frac{\partial \bsigma(C \cdot \bsigma(A \cdot x + b) + d))}{\partial C_{i,j}} \\
& = \frac{\partial \L(\y, \p)}{\partial \p}\bigg|_{\p=\bsigma(C \cdot \bsigma(A \cdot x + b) + d))} \ \frac{\partial \bsigma(\z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)}
\ \frac{\partial [C \cdot \bsigma(A \cdot x + b) + d]}{\partial C_{i,j}} \\
& = \frac{\partial \L(\y, \p)}{\partial \p}\bigg|_{\p=\bsigma(C \cdot \bsigma(A \cdot x + b) + d))} \ \frac{\partial \bsigma(\z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)}
\ [\bsigma(A \cdot x + b)]_j \cdot \e_i
\end{align*}

\begin{align*}
& \frac{\partial \cost(A,b,C,d)}{\partial d} \\
& = \frac{\partial \L(\y, f(A,b,C,d,x))}{\partial d}  \\
& = \frac{\partial \L(\y, \bsigma(C \cdot \bsigma(A \cdot x + b) + d))}{\partial d} \\
& = \frac{\partial \L(\y, \p)}{\partial \p}\bigg|_{\p=\bsigma(C \cdot \bsigma(A \cdot x + b) + d))} \ \frac{\partial \bsigma(C \cdot \bsigma(A \cdot x + b) + d))}{\partial d} \\
& = \frac{\partial \L(\y, \p)}{\partial \p}\bigg|_{\p=\bsigma(C \cdot \bsigma(A \cdot x + b) + d))} \ \frac{\partial \bsigma(\z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)}
\ \frac{\partial [C \cdot \bsigma(A \cdot x + b) + d]}{\partial d} \\
& = \frac{\partial \L(\y, \p)}{\partial \p}\bigg|_{\p=\bsigma(C \cdot \bsigma(A \cdot x + b) + d))} \ \frac{\partial \bsigma(\z)}{\partial \z}\bigg|_{\z=C \cdot \bsigma(A \cdot x + b) + d)}
\ I_{k \times k}
\end{align*}




\end{document}
